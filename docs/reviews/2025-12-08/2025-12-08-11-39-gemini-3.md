# Code Review: flappa-doormal
**Date:** 2025-12-08 11:39:14-05:00
**Reviewer:** Gemini3 (Google Deepmind)

## Executive Summary

The library provides a clean, declarative API for text segmentation, which is excellent for developer experience. However, the current implementation has **critical scalability issues** when dealing with large datasets ("tens of thousands of pages"). The core architecture relies on concatenating all pages into a single string, which leads to exponential performance degradation and excessive memory usage.

## Critical Findings

### 1. Scalability & Performance
**Severity: Critical**

The `segmentPages` function concatenates all input pages into a single string (`buildPageMap`).
- **Test Results**:
    - 10,000 pages (10MB): ~0.4s, ~20MB memory.
    - 50,000 pages (50MB): **~71s, ~1.2GB memory**.
- **Issue**: processing time and memory usage grow non-linearly. A 5x increase in input size resulted in a **177x increase in processing time** and **60x increase in memory usage**.
- **Cause**:
    - **Giant String**: Manipulating a 50MB+ string (concatenation, regex matching) puts immense pressure on the JS engine.
    - **Regex Backtracking**: Running complex regexes (especially with `.*` captures) on a massive string can cause catastrophic backtracking.
    - **Memory Overhead**: The 1.2GB memory usage for 50MB of text suggests a 24x overhead, likely due to string copying and internal object structures.

### 2. Regex Safety
**Severity: High**

The `lineStartsAfter` pattern implementation uses `^(?:patterns)(.*)`.
- **Issue**: The `(.*)` capture group is greedy. On a massive concatenated string, this forces the regex engine to match to the end of the string (or line) and backtrack. If the "line" is very long (or if `.` matches newlines in some configurations), this is a major performance bottleneck.
- **Risk**: This is a potential ReDoS (Regular Expression Denial of Service) vector if user content contains patterns that trigger worst-case regex behavior.

### 3. Fuzzy Matching Overhead
**Severity: Medium**

The `makeDiacriticInsensitive` function expands *every* character into a character class (e.g., `[اآأإ]`).
- **Issue**: A simple 10-character phrase becomes a massive regex pattern. When combined with `lineStartsAfter` and the giant string, this contributes significantly to the performance degradation.

## Assumptions & Limitations

1.  **Memory Availability**: The library assumes the entire dataset (plus significant overhead) fits in memory. This is risky for large books or serverless environments with memory limits.
2.  **Content Structure**: The library assumes `\n` is the only relevant separator after normalization.
3.  **Regex as the Hammer**: The library relies entirely on regex for segmentation. While flexible, it's not always the most efficient way to scan large texts, especially for simple markers.

## Recommendations

### 1. Architecture Refactor: Streaming/Chunking
**Priority: Immediate**

Instead of concatenating all pages:
- **Chunk Processing**: Process pages in smaller batches (e.g., 100 pages or by chapter).
- **Windowing**: If a segment can span pages, keep a "sliding window" of context (e.g., current page + previous page) rather than the whole book.
- **Iterator Pattern**: Change `segmentPages` to accept an iterator/generator of pages and yield segments lazily.

### 2. Optimize Regex
**Priority: High**

- **Non-Greedy Captures**: Change `(.*)` to `(.*?)` or explicitly exclude newlines `([^\n]*)` to prevent over-matching.
- **Anchor Optimization**: Ensure `^` and `$` anchors are working effectively on the per-line basis, not just the whole string.
- **Pre-compilation**: If `segmentPages` is called repeatedly, cache the compiled regexes derived from rules.

### 3. Fuzzy Matching Optimization
**Priority: Medium**

- **Normalization First**: Instead of expanding the regex to match all variations, consider normalizing the *input text* to a canonical form (stripping diacritics, unifying alefs) *before* matching. This allows using simple string comparisons or much simpler regexes.

### 4. Type Safety
**Priority: Low**

- The `PageInput` type is rigid. Consider making it generic or allowing a `getLine()` interface to decouple from the specific object structure.

## Conclusion

The library is well-designed for small-to-medium texts but is **not production-ready for "tens of thousands of pages"** in its current form. The concatenation strategy must be replaced with a streaming or chunked approach to achieve linear scalability.
